{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/envs/ml_python/lib/python3.5/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "import numpy as np\n",
    "import keras.backend.tensorflow_backend as K\n",
    "from common_helper import *\n",
    "from ord_helper import *\n",
    "from nltk_helper import *\n",
    "from keras_helper import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dr = \"../input/train.json\"\n",
    "test_dr = \"../input/test.json\"\n",
    "traindf, testdf = data_load(train_dr, test_dr)\n",
    "x_train, y_train = data_preprocessing(traindf)\n",
    "x_test, _ = data_preprocessing(testdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = data_vectorizer(x_train)\n",
    "x_test = data_vectorizer(x_test)\n",
    "\n",
    "x_train = [np.array(i) for i in x_train]\n",
    "x_test = [np.array(i) for i in x_test]\n",
    "x_train = np.array(x_train)\n",
    "x_test = np.array(x_test)\n",
    "\n",
    "lb = preprocessing.LabelBinarizer()\n",
    "lb.fit(traindf.cuisine.values)\n",
    "y_train = lb.transform(traindf.cuisine.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/strawberry/code/keras_helper.py:9: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(560, input_shape=(2000,), activation=\"selu\", kernel_initializer=\"glorot_uniform\")`\n",
      "  model.add(Dense(560, init='glorot_uniform', activation='selu', input_shape=(x_train.shape[1],)))\n",
      "/root/strawberry/code/keras_helper.py:11: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(280, activation=\"tanh\", kernel_initializer=\"glorot_uniform\")`\n",
      "  model.add(Dense(280, init='glorot_uniform', activation='tanh'))\n",
      "/root/strawberry/code/keras_helper.py:13: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(140, activation=\"selu\", kernel_initializer=\"glorot_uniform\")`\n",
      "  model.add(Dense(140, init='glorot_uniform', activation='selu'))\n",
      "/root/anaconda3/envs/ml_python/lib/python3.5/site-packages/keras/models.py:942: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  warnings.warn('The `nb_epoch` argument in `fit` '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 35796 samples, validate on 3978 samples\n",
      "Epoch 1/300\n",
      "35796/35796 [==============================] - 2s 47us/step - loss: 3.7217 - acc: 0.1682 - val_loss: 2.1560 - val_acc: 0.4155\n",
      "Epoch 2/300\n",
      "35796/35796 [==============================] - 1s 17us/step - loss: 2.6988 - acc: 0.3265 - val_loss: 1.7369 - val_acc: 0.5282\n",
      "Epoch 3/300\n",
      "35796/35796 [==============================] - 1s 17us/step - loss: 2.1848 - acc: 0.4297 - val_loss: 1.5034 - val_acc: 0.5714\n",
      "Epoch 4/300\n",
      "35796/35796 [==============================] - 1s 17us/step - loss: 1.9069 - acc: 0.4802 - val_loss: 1.3804 - val_acc: 0.6046\n",
      "Epoch 5/300\n",
      "35796/35796 [==============================] - 1s 17us/step - loss: 1.7442 - acc: 0.5174 - val_loss: 1.2838 - val_acc: 0.6264\n",
      "Epoch 6/300\n",
      "35796/35796 [==============================] - 1s 17us/step - loss: 1.6227 - acc: 0.5458 - val_loss: 1.2246 - val_acc: 0.6408\n",
      "Epoch 7/300\n",
      "35796/35796 [==============================] - 1s 17us/step - loss: 1.5299 - acc: 0.5665 - val_loss: 1.1996 - val_acc: 0.6471\n",
      "Epoch 8/300\n",
      "35796/35796 [==============================] - 1s 17us/step - loss: 1.4665 - acc: 0.5806 - val_loss: 1.1647 - val_acc: 0.6586\n",
      "Epoch 9/300\n",
      "35796/35796 [==============================] - 1s 17us/step - loss: 1.4037 - acc: 0.5948 - val_loss: 1.1274 - val_acc: 0.6694\n",
      "Epoch 10/300\n",
      "35796/35796 [==============================] - 1s 17us/step - loss: 1.3580 - acc: 0.6056 - val_loss: 1.1105 - val_acc: 0.6730\n",
      "Epoch 11/300\n",
      "35796/35796 [==============================] - 1s 17us/step - loss: 1.3176 - acc: 0.6147 - val_loss: 1.1074 - val_acc: 0.6750\n",
      "Epoch 12/300\n",
      "35796/35796 [==============================] - 1s 17us/step - loss: 1.2906 - acc: 0.6231 - val_loss: 1.1045 - val_acc: 0.6767\n",
      "Epoch 13/300\n",
      "35796/35796 [==============================] - 1s 17us/step - loss: 1.2549 - acc: 0.6329 - val_loss: 1.0645 - val_acc: 0.6885\n",
      "Epoch 14/300\n",
      "35796/35796 [==============================] - 1s 17us/step - loss: 1.2212 - acc: 0.6419 - val_loss: 1.0580 - val_acc: 0.6880\n",
      "Epoch 15/300\n",
      "35796/35796 [==============================] - 1s 17us/step - loss: 1.2000 - acc: 0.6498 - val_loss: 1.0591 - val_acc: 0.6888\n",
      "Epoch 16/300\n",
      "35796/35796 [==============================] - 1s 17us/step - loss: 1.1819 - acc: 0.6508 - val_loss: 1.0584 - val_acc: 0.6895\n",
      "Epoch 17/300\n",
      "35796/35796 [==============================] - 1s 17us/step - loss: 1.1549 - acc: 0.6551 - val_loss: 1.0384 - val_acc: 0.6963\n",
      "Epoch 18/300\n",
      "35796/35796 [==============================] - 1s 17us/step - loss: 1.1323 - acc: 0.6629 - val_loss: 1.0439 - val_acc: 0.6963\n",
      "Epoch 19/300\n",
      "35796/35796 [==============================] - 1s 17us/step - loss: 1.1109 - acc: 0.6717 - val_loss: 1.0415 - val_acc: 0.6963\n",
      "Epoch 20/300\n",
      "35796/35796 [==============================] - 1s 18us/step - loss: 1.0992 - acc: 0.6743 - val_loss: 1.0428 - val_acc: 0.6956\n",
      "Epoch 21/300\n",
      "35796/35796 [==============================] - 1s 17us/step - loss: 1.0897 - acc: 0.6748 - val_loss: 1.0402 - val_acc: 0.6905\n",
      "Epoch 22/300\n",
      "35796/35796 [==============================] - 1s 17us/step - loss: 1.0674 - acc: 0.6814 - val_loss: 1.0341 - val_acc: 0.6963\n",
      "Epoch 23/300\n",
      "35796/35796 [==============================] - 1s 17us/step - loss: 1.0588 - acc: 0.6838 - val_loss: 1.0280 - val_acc: 0.6961\n",
      "Epoch 24/300\n",
      "35796/35796 [==============================] - 1s 17us/step - loss: 1.0432 - acc: 0.6883 - val_loss: 1.0260 - val_acc: 0.7019\n",
      "Epoch 25/300\n",
      "35796/35796 [==============================] - 1s 17us/step - loss: 1.0357 - acc: 0.6919 - val_loss: 1.0299 - val_acc: 0.7001\n",
      "Epoch 26/300\n",
      "35796/35796 [==============================] - 1s 17us/step - loss: 1.0234 - acc: 0.6952 - val_loss: 1.0221 - val_acc: 0.7014\n",
      "Epoch 27/300\n",
      "35796/35796 [==============================] - 1s 17us/step - loss: 1.0102 - acc: 0.6963 - val_loss: 1.0341 - val_acc: 0.6943\n",
      "Epoch 28/300\n",
      "35796/35796 [==============================] - 1s 18us/step - loss: 0.9995 - acc: 0.7002 - val_loss: 1.0397 - val_acc: 0.6971\n",
      "Epoch 29/300\n",
      "35796/35796 [==============================] - 1s 17us/step - loss: 0.9949 - acc: 0.7009 - val_loss: 1.0245 - val_acc: 0.6998\n",
      "Epoch 30/300\n",
      "35796/35796 [==============================] - 1s 17us/step - loss: 0.9821 - acc: 0.7060 - val_loss: 1.0215 - val_acc: 0.7006\n",
      "Epoch 31/300\n",
      "35796/35796 [==============================] - 1s 17us/step - loss: 0.9732 - acc: 0.7075 - val_loss: 1.0269 - val_acc: 0.7021\n",
      "Epoch 32/300\n",
      "35796/35796 [==============================] - 1s 18us/step - loss: 0.9693 - acc: 0.7071 - val_loss: 1.0205 - val_acc: 0.7026\n",
      "Epoch 33/300\n",
      "35796/35796 [==============================] - 1s 17us/step - loss: 0.9523 - acc: 0.7143 - val_loss: 1.0247 - val_acc: 0.7004\n",
      "Epoch 34/300\n",
      "35796/35796 [==============================] - 1s 17us/step - loss: 0.9455 - acc: 0.7162 - val_loss: 1.0416 - val_acc: 0.6966\n",
      "Epoch 35/300\n",
      "35796/35796 [==============================] - 1s 17us/step - loss: 0.9519 - acc: 0.7136 - val_loss: 1.0194 - val_acc: 0.6981\n",
      "Epoch 36/300\n",
      "35796/35796 [==============================] - 1s 17us/step - loss: 0.9405 - acc: 0.7154 - val_loss: 1.0318 - val_acc: 0.6976\n",
      "Epoch 37/300\n",
      "35796/35796 [==============================] - 1s 17us/step - loss: 0.9258 - acc: 0.7218 - val_loss: 1.0423 - val_acc: 0.6978\n",
      "Epoch 38/300\n",
      "35796/35796 [==============================] - 1s 18us/step - loss: 0.9286 - acc: 0.7197 - val_loss: 1.0302 - val_acc: 0.6986\n",
      "Epoch 39/300\n",
      "35796/35796 [==============================] - 1s 17us/step - loss: 0.9270 - acc: 0.7213 - val_loss: 1.0211 - val_acc: 0.7009\n",
      "Epoch 40/300\n",
      "35796/35796 [==============================] - 1s 17us/step - loss: 0.9125 - acc: 0.7244 - val_loss: 1.0225 - val_acc: 0.7046\n",
      "Epoch 41/300\n",
      "35796/35796 [==============================] - 1s 17us/step - loss: 0.9056 - acc: 0.7271 - val_loss: 1.0242 - val_acc: 0.7019\n",
      "Epoch 42/300\n",
      "35796/35796 [==============================] - 1s 17us/step - loss: 0.9112 - acc: 0.7249 - val_loss: 1.0400 - val_acc: 0.7019\n",
      "Epoch 43/300\n",
      "35796/35796 [==============================] - 1s 17us/step - loss: 0.8949 - acc: 0.7304 - val_loss: 1.0452 - val_acc: 0.6971\n",
      "Epoch 44/300\n",
      "35796/35796 [==============================] - 1s 17us/step - loss: 0.8987 - acc: 0.7272 - val_loss: 1.0359 - val_acc: 0.6978\n",
      "Epoch 45/300\n",
      "35796/35796 [==============================] - 1s 17us/step - loss: 0.8876 - acc: 0.7299 - val_loss: 1.0377 - val_acc: 0.6983\n",
      "Epoch 46/300\n",
      "35796/35796 [==============================] - 1s 17us/step - loss: 0.8811 - acc: 0.7336 - val_loss: 1.0309 - val_acc: 0.6968\n",
      "Epoch 47/300\n",
      "35796/35796 [==============================] - 1s 17us/step - loss: 0.8798 - acc: 0.7345 - val_loss: 1.0446 - val_acc: 0.6951\n",
      "Epoch 48/300\n",
      "35796/35796 [==============================] - 1s 17us/step - loss: 0.8677 - acc: 0.7377 - val_loss: 1.0429 - val_acc: 0.7006\n",
      "Epoch 49/300\n",
      "35796/35796 [==============================] - 1s 18us/step - loss: 0.8700 - acc: 0.7372 - val_loss: 1.0379 - val_acc: 0.6943\n",
      "Epoch 50/300\n",
      "35796/35796 [==============================] - 1s 17us/step - loss: 0.8626 - acc: 0.7347 - val_loss: 1.0362 - val_acc: 0.6981\n",
      "Epoch 51/300\n",
      "35796/35796 [==============================] - 1s 17us/step - loss: 0.8639 - acc: 0.7378 - val_loss: 1.0470 - val_acc: 0.6936\n",
      "Epoch 52/300\n",
      "35796/35796 [==============================] - 1s 17us/step - loss: 0.8606 - acc: 0.7377 - val_loss: 1.0640 - val_acc: 0.6938\n",
      "Epoch 53/300\n",
      "35796/35796 [==============================] - 1s 17us/step - loss: 0.8628 - acc: 0.7384 - val_loss: 1.0380 - val_acc: 0.6958\n",
      "Epoch 54/300\n",
      "35796/35796 [==============================] - 1s 17us/step - loss: 0.8562 - acc: 0.7403 - val_loss: 1.0504 - val_acc: 0.6946\n",
      "Epoch 55/300\n",
      "35796/35796 [==============================] - 1s 18us/step - loss: 0.8530 - acc: 0.7397 - val_loss: 1.0467 - val_acc: 0.6956\n",
      "Epoch 56/300\n",
      "35796/35796 [==============================] - 1s 17us/step - loss: 0.8452 - acc: 0.7429 - val_loss: 1.0520 - val_acc: 0.6986\n",
      "Epoch 57/300\n",
      "35796/35796 [==============================] - 1s 17us/step - loss: 0.8392 - acc: 0.7457 - val_loss: 1.0562 - val_acc: 0.6956\n",
      "Epoch 58/300\n",
      "35796/35796 [==============================] - 1s 17us/step - loss: 0.8363 - acc: 0.7453 - val_loss: 1.0523 - val_acc: 0.6948\n",
      "Epoch 59/300\n",
      "35796/35796 [==============================] - 1s 17us/step - loss: 0.8315 - acc: 0.7459 - val_loss: 1.0599 - val_acc: 0.6973\n",
      "Epoch 60/300\n",
      "35796/35796 [==============================] - 1s 18us/step - loss: 0.8272 - acc: 0.7437 - val_loss: 1.0594 - val_acc: 0.6958\n",
      "Epoch 61/300\n",
      "35796/35796 [==============================] - 1s 17us/step - loss: 0.8256 - acc: 0.7487 - val_loss: 1.0523 - val_acc: 0.7009\n",
      "Epoch 62/300\n",
      "35796/35796 [==============================] - 1s 17us/step - loss: 0.8254 - acc: 0.7466 - val_loss: 1.0750 - val_acc: 0.6913\n",
      "Epoch 63/300\n",
      "35796/35796 [==============================] - 1s 17us/step - loss: 0.8285 - acc: 0.7471 - val_loss: 1.0736 - val_acc: 0.6936\n",
      "Epoch 64/300\n",
      "35796/35796 [==============================] - 1s 17us/step - loss: 0.8245 - acc: 0.7484 - val_loss: 1.0679 - val_acc: 0.6963\n",
      "Epoch 65/300\n",
      "35796/35796 [==============================] - 1s 17us/step - loss: 0.8227 - acc: 0.7466 - val_loss: 1.0575 - val_acc: 0.6966\n",
      "Epoch 66/300\n",
      "35796/35796 [==============================] - 1s 17us/step - loss: 0.8200 - acc: 0.7502 - val_loss: 1.0538 - val_acc: 0.6941\n",
      "Epoch 67/300\n",
      "35796/35796 [==============================] - 1s 18us/step - loss: 0.8112 - acc: 0.7512 - val_loss: 1.0513 - val_acc: 0.6963\n",
      "Epoch 68/300\n",
      "35796/35796 [==============================] - 1s 17us/step - loss: 0.8070 - acc: 0.7526 - val_loss: 1.0661 - val_acc: 0.6946\n",
      "Epoch 69/300\n",
      "35796/35796 [==============================] - 1s 17us/step - loss: 0.8051 - acc: 0.7524 - val_loss: 1.0732 - val_acc: 0.6931\n",
      "Epoch 70/300\n",
      "35796/35796 [==============================] - 1s 17us/step - loss: 0.8040 - acc: 0.7563 - val_loss: 1.0621 - val_acc: 0.7011\n",
      "Epoch 71/300\n",
      "35796/35796 [==============================] - 1s 17us/step - loss: 0.8016 - acc: 0.7537 - val_loss: 1.0646 - val_acc: 0.6956\n",
      "Epoch 72/300\n",
      "35796/35796 [==============================] - 1s 17us/step - loss: 0.7990 - acc: 0.7551 - val_loss: 1.0659 - val_acc: 0.6991\n",
      "Epoch 73/300\n",
      "35796/35796 [==============================] - 1s 17us/step - loss: 0.7966 - acc: 0.7568 - val_loss: 1.0692 - val_acc: 0.6976\n",
      "Epoch 74/300\n",
      "35796/35796 [==============================] - 1s 17us/step - loss: 0.7945 - acc: 0.7550 - val_loss: 1.0711 - val_acc: 0.6988\n",
      "Epoch 75/300\n",
      "35796/35796 [==============================] - 1s 18us/step - loss: 0.7885 - acc: 0.7567 - val_loss: 1.0769 - val_acc: 0.6976\n",
      "Epoch 76/300\n",
      " 4000/35796 [==>...........................] - ETA: 0s - loss: 0.8071 - acc: 0.7513"
     ]
    }
   ],
   "source": [
    "with K.tf.device('/gpu:0'):\n",
    "    mdl = load_model(x_train)\n",
    "    mdl.fit(x_train, y_train, nb_epoch=300, batch_size=4000,validation_split=0.1)\n",
    "    y_test = mdl.predict(x_test)     \n",
    "    predictions = lb.inverse_transform(y_test)\n",
    "    save_submission(testdf, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
